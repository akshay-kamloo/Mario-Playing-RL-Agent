# Marioâ€‘RL Agent ğŸ•¹ï¸ğŸ¢ â€” Playing **SuperÂ MarioÂ Bros.** with DoubleÂ DQN

A researchâ€‘grade reinforcementâ€‘learning agent that learns to clear *SuperÂ MarioÂ Bros.* levels using a **Double DeepÂ Qâ€‘Network (DDQN)**.  
The project began as my final project for *ITCSÂ 5156 â€“ Applied Machine Learning* at UNCÂ Charlotte and explores how careful state abstraction, frame preprocessing, and targetâ€‘network stabilisation make deep RL tractable for classic platformers.

---

## Demo

| Type | Link |
|------|------|
| ğŸ” **Notebook** | [`MARIO_RL.ipynb`](./MARIO_RL.ipynb) |

---

## Table of Contents
1. [Features](#features)
2. [QuickÂ Start](#quick-start)
3. [ProjectÂ Structure](#project-structure)
4. [Methodology](#methodology)
   - [EnvironmentÂ Wrappers](#environment-wrappers)
   - [NetworkÂ Architecture](#network-architecture)
   - [TrainingÂ Regimen](#training-regimen)
5. [Results](#results)
6. [FutureÂ Work](#future-work)
7. [Contributing](#contributing)
8. [License](#license)
9. [Acknowledgements](#acknowledgements)

---

## Features
- **DoubleÂ DQN training loop** with separate *online* and *target* networks for stable Qâ€‘updates.  
- **Frameâ€‘stacked, grayscale, downâ€‘sampled state** (`4Â Ã—Â 84Â Ã—Â 84`) for sampleâ€‘efficient learning.  
- **Custom Gym wrappers** (`SkipFrame`, `GrayScaleObservation`, `ResizeObservation`, `FrameStack`) to shrink the observation space and skip redundant frames.  
- Replay buffer with *uniform random sampling* and an *epsilonâ€‘greedy* exploration schedule.  
- Fully selfâ€‘contained **Jupyter notebook**â€”run, visualise, and tweak every component in one place.  
- Plugâ€‘andâ€‘play for other **NESÂ Gym** environments; only the action mapping changes.

---

## QuickÂ Start

### 1. Clone & create env (ğŸÂ PythonÂ 3.9+)

```bash
git clone https://github.com/<yourâ€‘handle>/marioâ€‘rl.git
cd marioâ€‘rl
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

<details>
<summary><code>requirements.txt</code></summary>

```
gym-super-mario-bros==7.4.0
nes-py==8.2.1
torch>=2.2
torchvision
numpy
opencv-python
matplotlib
tqdm
```
</details>

### 2. Run the notebook

```bash
jupyter notebook MARIO_RL.ipynb
```

> **TipÂ ğŸ’¡**â€ƒPrefer a pureâ€‘Python script? Export the notebook (`jupyter nbconvert --to script`) or adapt the cells into `train.py`.

---

## ProjectÂ Structure

```
marioâ€‘rl/
â”‚
â”œâ”€â”€ MARIO_RL.ipynb        # endâ€‘toâ€‘end notebook (dataâ€‘prep â†’ training â†’ eval)
   â””â”€â”€ agents/
   â”‚   â””â”€â”€ ddqn.py           # DDQN implementation (actor, optimiser, buffer)
   â””â”€â”€ wrappers/
   â”‚   â””â”€â”€ mario_wrappers.py # SkipFrame, GrayScaleObservation, ResizeObservation, ...
   â””â”€â”€ assets/
   â”‚   â”œâ”€â”€ reward_curve.png  # training curves
   â”‚   â””â”€â”€ demo.gif          # short gameplay GIF
   â””â”€â”€ requirements          # in the beginning
â””â”€â”€ README.md
```

Feel free to reorganiseâ€”paths inside the notebook use relative imports.

---

## Methodology

### EnvironmentÂ Wrappers
| Wrapper | Purpose | Output Shape |
|---------|---------|--------------|
| `GrayScaleObservation` | Drop RGB channels â†’ memory & compute â†“ | `1Â Ã—Â 240Â Ã—Â 256` |
| `ResizeObservation` | Downâ€‘sample to square frame | `1Â Ã—Â 84Â Ã—Â 84` |
| `SkipFrame(n=4)` | Return every *n*â€‘th frame, accumulate rewards | â€” |
| `FrameStack(k=4)` | Stack *k* past frames â†’ motion awareness | `4Â Ã—Â 84Â Ã—Â 84` |

Together these wrappers condense the raw NES screen into a manageable **28Â kB** tensor while preserving temporal dynamics.

### NetworkÂ Architecture
```
Input: 4 Ã— 84 Ã— 84
â”‚
â”œâ”€ CNN(32 @ 8Ã—8, strideÂ 4) â†’ ReLU
â”œâ”€ CNN(64 @ 4Ã—4, strideÂ 2) â†’ ReLU
â”œâ”€ CNN(64 @ 3Ã—3, strideÂ 1) â†’ ReLU
â”‚            â””â”€â”€â–º flatten â†’ FC(512) â†’ ReLU
â””â”€ FC(|A|)  # |A|Â =Â number of discrete actions
```
The *target* network weights are hardâ€‘copied from the *online* network every **1â€¯000** steps to mitigate value blowâ€‘up.

### TrainingÂ Regimen
| Hyperâ€‘parameter | Value |
|-----------------|-------|
| Episodes | 40â€¯000 |
| Replay Capacity | 100â€¯000 |
| Minibatch Size | 32 |
| Optimiser | Adam (lrÂ =Â 1eâ€‘4) |
| Discount Î³ | 0.99 |
| Exploration Ïµ | 1.0 â†’ 0.05 (linear decay over 1â€¯M steps) |

Average return plateaus after ~10â€¯k episodes; levelâ€‘completion rate peaks closer to 40â€¯k.

---

## Results

![Reward curve](reward_cur.jpg)

- **Convergence**: returns stabilise rapidly; extra training chiefly improves *survival time*.  
- **Sample efficiency**: DDQN needs ordersâ€‘ofâ€‘magnitude fewer frames than vanilla DQN on identical hardware.  
- **Limitations**: only two discrete actions were trained (RIGHT, JUMP) due to compute constraintsâ€”powerâ€‘ups and backward movement are ignored.

---

## FutureÂ Work
- Expand action space (LEFT, FIRE, SPEED) and train across multiple level seeds.  
- Reward shaping for coin collection / enemy stomps to create *aggressive* or *treasureâ€‘hunter* behaviours.  
- **Prioritised replay** or **Rainbow DQN** improvements for better sample efficiency.  
- Port to **Gymnasium** and â˜• **Java MarioÂ AI Benchmark** for crossâ€‘environment benchmarking.

---

## Contributing

Bug reports and pull requests are welcome! Please open an issue describing:

1. What went wrong or what feature you propose  
2. Steps to reproduce / design sketch  
3. Environment (Python version, CUDA, etc.)

Then fork â†’ feature branch â†’ PR.

---

## License

This project is released under the **MIT License**â€”see [`LICENSE`](./LICENSE) for details.

---

## Acknowledgements
- H.Â vanÂ Hasselt, A.Â Guez, D.Â SilverÂ â€” *Deep RL with Double Qâ€‘Learning* (AAAIÂ 2016)  
- OpenAI Gym & `gymâ€‘superâ€‘marioâ€‘bros` community  
- UNCÂ Charlotte ITCSÂ 5156 course & Dr.Â Lee for project guidance  

*Made with â¤ï¸ and far too many Goombas.*
